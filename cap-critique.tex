\documentclass[a4paper,twocolumn,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx} % times roman, including math
\usepackage[hyphens]{url}
\usepackage{doi}
\usepackage{hyperref}
\usepackage[numbers,sort]{natbib}
\frenchspacing

% People to send this to for review: Henry Robinson, Peter Bailis, Nicolas Liochon, Justin Sheehy
\begin{document}
\sloppy
\date{} % No date
\title{A Critique of the CAP Theorem}
\author{Martin Kleppmann \and Niklas Ekstr{\"o}m}
\maketitle

\subsection*{Abstract}

The so-called \emph{CAP Theorem} is a frequently cited impossibility result in distributed systems,
especially among the new generation of \emph{NoSQL} distributed databases. In this paper we show
that the proof of CAP is incorrect, due to problems in the way it is formalized. We therefore
propose that it should be referred to as \emph{CAP Principle} rather than \emph{CAP Theorem}.
However, the general trade-off between replica consistency and sensitivity to network problems is
real, and in this paper we discuss some alternative results from distributed systems theory that can
help practitioners reason about trade-offs in their systems.

\section{Background}

Replicated databases maintain copies of the same data on multiple nodes, potentially in disparate
geographical locations, in order to tolerate faults (failures of nodes or communication links) and
to provide lower latency to users (requests can be served by a nearby site). However, implementing
reliable, fault-tolerant applications in a distributed system is difficult: if there are multiple
copies of the data on different nodes, they may be inconsistent with each other, and an application
that is not designed to handle such inconsistencies may produce incorrect results.

In order to provide a simpler programming model to application developers, the designers of
distributed data systems have explored various consistency guarantees. Distributed consistency
models such as linearizability~\cite{Herlihy1990jq}, sequential consistency~\cite{Lamport1979ky},
causal consistency~\cite{Ahamad1995gl} and pipelined RAM (PRAM)~\cite{Lipton1988uh} encapsulate
certain guarantees about the propagation of writes between the replicas of the database, and thus
permit the programmer to ignore the problem of replica divergence to some degree. By analogy,
transaction isolation models such as serializability, snapshot isolation~\cite{Berenson1995kj},
repeatable read and read committed~\cite{Gray1976us} describe certain guarantees about concurrently
executing transactions, and permit the programmer to ignore the problem of race conditions to some
degree~\cite{Bailis2014vc}.

A strong consistency model like linearizability provides an easy-to-understand guarantee:
informally, all operations behave as if they executed atomically on a \emph{single copy} of the
data. However, this guarantee comes at the cost of reduced performance~\cite{Attiya1994gw} and fault
tolerance~\cite{Davidson1985hv} compared to weaker consistency models. In particular, as we discuss
in this paper, algorithms that ensure a strong consistency property among replicas are sensitive to
message delays and faults in the network. Many real computer networks are prone to unbounded delays
and dropped messages~\cite{Bailis2014jx}, making the fault tolerance of distributed consistency
algorithms an important issue in practice.

A particular kind of fault that has been studied since the 1970s is the \emph{network partition}, a
communication fault that splits the network into subsets of nodes such that nodes in one subset
cannot communicate with nodes in another~\cite{Johnson1975we, Lindsay1979wv, Davidson1985hv}. As
long as the partition exists, any data modifications made in one subset of nodes cannot be visible
to nodes in another subset, since all messages between them are lost. Thus, an algorithm that
maintains the illusion of a single copy may have to delay operations until the partition is healed,
to avoid the risk of introducing inconsistent data in different subsets.

This trade-off was already known in 1975~\cite{Johnson1975we}, but it was rediscovered in the early
2000s, when the web's growing commercial popularity made geographic distribution and high
availability important to many organizations~\cite{Brewer2012tr}. It was originally called the
\emph{CAP Principle} by Fox and Brewer~\cite{Fox1999bs, Brewer2000vd}, where CAP stands for
\emph{Consistency}, \emph{Availability} and \emph{Partition tolerance}. After the principle was
formalized by Gilbert and Lynch~\cite{Gilbert2002il, Gilbert2012bf} it became known as the
\emph{CAP Theorem}.

CAP was adopted by distributed systems practitioners to critique design
decisions~\cite{Hodges2013tj}, and became a rallying cry of the NoSQL movement~\cite{Brewer2012ba}.
It provoked a lively debate about trade-offs in data systems, partly due to its definitions of
consistency, availability and partition tolerance being unclear~\cite{Robinson2010tp}. We will
examine these definitions in section~\ref{sec:definitions}.

\section{CAP Theorem Definitions}\label{sec:definitions}

CAP was originally presented in the form of \emph{``consistency, availability, partition tolerance:
pick any two''} (i.e.\ you can have CA, CP or AP, but not all three). Subsequent debates concluded
that this formulation is misleading, since the distinction between CA and CP is
unclear~\cite{Brewer2012ba, Hale2010we, Robinson2010tp}. Many authors now prefer the following
formulation: if there is no network partition, a system can be both consistent and available; when a
network partition occurs, a system must choose between either consistency (CP) or availability (AP).

Some authors~\cite{Darcy2010ta, Liochon2015vt} define a CP system as one in which a majority of
nodes on one side of a partition can continue operating normally, and a CA system as one that may
fail catastrophically under a network partition. However, this definition is not universally agreed,
since it is counterintuitive to label a system as ``available'' if it fails catastrophically under a
partition, while a system that continues partially operating in the same situation is labelled
``unavailable'' (see section~\ref{sec:partitions}).

Disagreement about the definitions of terms like \emph{availability} is the source of many
misunderstandings about CAP. Unclear definitions also sabotage attempts to formalize the
consistency/availability trade-off as a theorem. Indeed, there is considerable divergence between
the informal CAP principle laid out by Brewer, and the formalization by Gilbert and Lynch.

\subsection{Availability}\label{sec:availability}

In practical engineering terms, \emph{availability} usually refers to the proportion of time during
which a service is able to successfully handle requests, or the proportion of requests that receive
a successful response. A response is usually considered successful if it is valid (not an error, and
satisfies the database's safety properties) and it arrives at the client within some timeout, which
may be specified in a \emph{service level agreement} (SLA). Availability in this sense is a metric
that is empirically observed during a period of a service's operation. A service may be available
(up) or unavailable (down) at any given time, but it doesn't make sense to say that some software
package or algorithm is `available' or `unavailable' in general, since the uptime percentage is
only known in retrospect, after a period of operation (during which various faults may have
occurred).

There is a long tradition of \emph{highly available} and \emph{fault-tolerant} systems, whose
algorithms are designed such that the system can remain available (up) even when some part of the
system is faulty, thus increasing the expected mean time to failure (MTTF) of the system as a whole.
Using such a system does not automatically make a service 100\% available, but it may increase the
observed availability during operation, compared to using a system that is not fault-tolerant.

\subsubsection{The A in CAP}\label{sec:a-in-cap}

Does the A in CAP refer to a property of a system design, or to an observed metric during system
operation? This distinction is unclear. Brewer does not offer a precise definition of availability,
but states that ``availability is obviously continuous from 0 to 100 percent''~\cite{Brewer2012ba},
suggesting an observed metric. Fox and Brewer also use the term \emph{yield} to refer to the
proportion of requests that are completed successfully~\cite{Fox1999bs} (without specifying any
timeout).

On the other hand, Gilbert and Lynch~\cite{Gilbert2002il} write: ``For a distributed system to be
continuously available, every request received by a non-failing node in the system must result in a
response''. (This sentence appears to define a property of \emph{continuous availability}, but the
rest of the paper does not refer to this ``continuous'' aspect.) In order to prove a result about
systems in general, this definition interprets availability as a binary property of a system's
design, not as an observed metric during system operation --- i.e.\ they define a system as being
``available'' or ``unavailable'' based on its algorithms, not its operational status at some point
in time.

Note that Gilbert and Lynch's definition requires \emph{any} non-failed node to be able to generate
valid responses, even if that node is completely isolated from the other nodes. This definition is
at odds with Fox and Brewer's original proposal of CAP, which states that ``data is considered
highly available if a given consumer of the data can always reach \emph{some}
replica''~\cite[emphasis original]{Fox1999bs}.

Gilbert and Lynch's requirement appears to be stronger, since Fox and Brewer's definition allows a
node to refuse requests as long as another node can generate valid responses, and that node is
reachable by the consumer. Indeed, many so-called highly available or fault-tolerant systems are
``unavailable'' under Gilbert and Lynch's definition, even though they may have very high uptime in
practice.

\subsubsection{Failed nodes}\label{sec:failed-node-exception}

Another curious aspect of Gilbert and Lynch's definition of availability is the proviso of applying
only to \emph{non-failed} nodes. This allows the aforementioned definition of a CA system as one
that fails catastrophically if a network partition occurs: if the partition causes all nodes to
fail, then the availability requirement does not apply to any nodes, and thus it is trivially
satisified, even if no node is able to respond to any requests. This is logically consistent, but a
somewhat counter-intuitive definition of availability.

\subsection{Consistency}\label{sec:consistency}

Consistency is also an overloaded word in data systems: consistency in the sense of ACID is a very
different property from consistency in CAP~\cite{Brewer2012ba}. In the distributed systems
literature, consistency is usually understood as not one particular property, but as a range of
properties with varying strengths of guarantee. Examples of such consistency models include
linearizability~\cite{Herlihy1990jq}, sequential consistency~\cite{Lamport1979ky}, causal
consistency~\cite{Ahamad1995gl} and PRAM~\cite{Lipton1988uh}.

There is some overlap between distributed consistency models (which pertain to systems in which
there are multiple replicas of the data) and transaction isolation models (which pertain to systems
in which there are multiple concurrently executing transactions, usually assuming a single replica).
Transaction isolation models include serializability~\cite{Bernstein1987va}, snapshot
isolation~\cite{Berenson1995kj}, repeatable read and read committed~\cite{Gray1976us}. Bailis et
al.~\cite{Bailis2014vc} demonstrate a unified framework for reasoning about both distributed
consistency and transaction isolation in terms of CAP.

\subsubsection{The C in CAP}\label{sec:c-in-cap}

Fox and Brewer~\cite{Fox1999bs} define the C in CAP as one-copy serializability
(1SR)~\cite{Bernstein1987va}, whereas Gilbert and Lynch~\cite{Gilbert2002il} define it as
linearizability. Those definitions are not identical, but fairly similar.\footnote{Linearizability
is a recency guarantee, whereas 1SR is not. 1SR requires isolated execution of multi-object
transactions, which linearizability does not. Both are ``unavailable'' in the CAP
sense~\cite{Bailis2014vc}.} Both are safety properties~\cite{Alpern1985dg}, i.e.\ restrictions on
the possible executions of the system, ensuring that certain situations never occur.

In the case of linearizability, the situation that may not occur is a \emph{stale read}: stated
informally, if read operation $r$ begins after write operation $w$ has completed, and there are no
intervening writes, then $r$ must return the value written by $w$. Gilbert and Lynch observe that if
$r$ and $w$ occur on different nodes, and those nodes cannot communicate during the time when those
operations are being executed, then the safety property cannot be satisfied, because $r$ cannot know
about the value of $w$.

The C of CAP is sometimes referred to as \emph{strong consistency} (a term that is not formally
defined), and contrasted with \emph{eventual consistency}~\cite{Terry1994fp}, which is often
regarded as the weakest level of consistency that is useful to applications. Eventual consistency
means that if a system stops accepting writes and sufficient\footnote{It is not clear what amount of
communication is `sufficient'. A possible formalization would be to require all replicas to converge
to the same value within finite time, assuming fair-loss links (see section \ref{sec:fairloss}).}
communication occurs, then eventually all replicas will converge to the same value. However, as the
aforementioned list of consistency models indicates, it is overly simplistic to cast `strong' and
eventual consistency as the only possible choices.

\subsubsection{Probabilistic consistency}

It is also possible to define consistency as a quantitative metric rather than a safety property.
For example, Fox and Brewer~\cite{Fox1999bs} define \emph{harvest} as ``the fraction of the data
reflected in the response, i.e.\ the completeness of the answer to the query,'' and
probabilistically bounded staleness~\cite{Bailis2012to} studies the probability of a read returning
a stale value, given various assuptions about the distribution of network latencies. However, these
stochastic definitions of consistency are not the subject of CAP.

\subsection{Partition Tolerance}\label{sec:partitions}

A \emph{network partition} has long been defined as a communication failure in which the network is
split into disjoint sub-networks, with no communication possible across
sub-networks~\cite{Johnson1975we}. This is a fairly narrow class of fault, but it does occur in
practice~\cite{Bailis2014jx}, so it is worth studying.

\subsubsection{Assumptions about system model}

It is less clear what \emph{partition tolerance} means. Gilbert and Lynch~\cite{Gilbert2002il}
define a system as partition-tolerant if it continues to satisfy the consistency and availability
properties in the presence of a partition. Fox and Brewer~\cite{Fox1999bs} define
\emph{partition-resilience} as ``the system as whole can survive a partition between data replicas''
(where \emph{survive} is not defined).

At first glance, these definitions may seem redundant: if we say that an algorithm provides some
guarantee (e.g. linearizability), then we expect \emph{all} executions of the algorithm to satisfy
that property, regardless of the faults that occur during the execution.

However, we can clarify the definitions by observing that the correctness of a distributed algorithm
is always subject to assumptions about the faults that may occur during its execution. If you take
an algorithm that assumes fair-loss links and crash-stop processes, and subject it to Byzantine
faults, the execution will most likely violate safety properties that were supposedly guaranteed.
Similarly, we can interpret \emph{partition tolerance} as meaning ``a network partition is among the
faults that are assumed to be possible in the system.''

Note that this definition of partition tolerance is a statement of assumptions about the underlying
system model. This makes it different from consistency and availability, which are properties of the
possible executions of an algorithm. It makes no sense to say that an algorithm ``provides partition
tolerance,'' we can only say that an algorithm ``assumes that partitions may occur.''

If an algorithm assumes the absence of partitions, and is nevertheless subjected to a partition, it
may violate its guarantees in arbitrarily undefined ways (including failing to respond even after
the partition is repaired, or deleting arbitrary amounts of data). Although it may seem that such
arbitrary failure semantics are not very useful, various systems exhibit such behavior in
practice~\cite{Kingsbury2014tk, Kingsbury2015uk}.

It has been suggested that partition tolerance should instead be phrased as ``possibility of
partitions occurring''~\cite{Robinson2010tp}, which makes clear that it is not a property of an
algorithm, but a property of the underlying infrastructure. In practice, making networks highly
reliable is very expensive~\cite{Bailis2014jx}, so most distributed programs must assume that
partitions will occur sooner or later~\cite{Hale2010we}.

\subsubsection{Partitions and fair-loss links}\label{sec:fairloss}

Further confusion arises due to the fact that network partitions are only one of a wide range of
faults that can occur in distributed systems, including nodes failing or restarting, nodes pausing
for some amount of time (e.g.\ due to garbage collection), and loss or delay of messages in the
network. Some faults can be modeled in terms of other faults (for example, Gilbert and Lynch state
that the loss of an individual message can be modeled as a short-lived network partition).

In the design of distributed systems algorithms, a commonly assumed system model is \emph{fair-loss
links}~\cite{Cachin2011wt}. A network link has the fair-loss property if the probability of a
message \emph{not} being lost is non-zero, i.e.\ the link sometimes delivers messages, and it does
not systematically drop every message. On a fair-loss link, message delivery can be made reliable by
retrying a message an unbounded number of times: the message is guaranteed to be eventually
delivered after a finite number of attempts~\cite{Cachin2011wt}.

Arguably, fair-loss links are a good model of networks in practice: faults occur randomly; messages
are lost while the fault is occurring; the fault lasts for some finite duration (perhaps seconds,
perhaps hours), and eventually it is repaired (perhaps after human intervention). There is no
malicious actor in the network who can cause systematic message loss over unlimited periods of time
-- such malicious actors are only assumed in the design of Byzantine fault tolerant algorithms.

Is ``possibility of partitions occurring'' equivalent to assuming fair-loss links? Gilbert and
Lynch~\cite{Gilbert2002il} define partitions as ``the network will be allowed to lose arbitrarily
many messages sent from one node to another.'' In this definition it is unclear whether the number
of lost messages is unbounded but finite, or whether it is potentially infinite.

Partitions of a finite duration are possible with fair-loss links, and thus an algorithm that is
correct in a system model of fair-loss links can tolerate partitions of a finite duration.
Partitions of an infinite duration require some further thought, as we shall see in
section~\ref{sec:proofs}.

\section{The CAP Proofs}\label{sec:proofs}

In this section, we build upon the discussion of definitions in the last section, and examine the
proofs of the purported theorems and corollary of Gilbert and Lynch~\cite{Gilbert2002il}
(collectively known as the ``proof of the CAP theorem''). We demonstrate that each of these proofs
is false.

\subsection{Theorem 1}\label{sec:theorem1}

Gilbert and Lynch's Theorem 1 is stated as follows:

\emph{It is impossible in the asynchronous network model to implement a read/write data object that
guarantees the following properties:}
\begin{itemize}
\item \emph{Availability}
\item \emph{Atomic consistency}\footnote{\emph{Atomic consistency} is synonymous with linearizability, and
it is unrelated to the A in ACID.}
\end{itemize}
\emph{in all fair executions (including those in which messages are lost).}

\subsubsection{Counterexample}

The simplest way of showing Theorem 1 to be false is by giving a counterexample. The ABD algorithm,
published by Attiya, Bar-Noy and Dolev five years before CAP was proposed~\cite{Attiya1995bm},
implements an atomic (linearizable) read-write register\footnote{ABD~\cite{Attiya1995bm} is an
algorithm for a single-writer multi-reader register. It was extended to the multi-writer case by
Lynch and Shvartsman~\cite{Lynch1997gr}.} in an unreliable asynchronous network, while also
satisfying Gilbert and Lynch's definition of availability.

If ABD is used with fair-loss links, communication is made reliable by retrying messages an
unbounded number of times until they are delivered. Under this assumption, ABD executions of
\emph{read} and \emph{write} operations always terminate provided that a majority of nodes is not
faulty. As the CAP definition of availability only requires operations to terminate (there is no
limit on the response time, as long as it is finite), this meets the CAP definition of availability.

If nodes are permanently partitioned, or a majority of nodes fails, ABD employs a simple solution:
any node that cannot communicate with a majority of nodes is deemed faulty (for example, it crashes
itself). Under the CAP definition of availability, a faulty node is not required to respond to
requests (section~\ref{sec:failed-node-exception}). There is nothing in the definition of CAP that
prohibits nodes from becoming crashed due to network problems.

Defining a minority partition as failed may feel like cheating: the intuition of CAP availability is
that a minority partition should be able to continue processing. However, Gilbert and Lynch's
definition of availability does not capture this intuition correctly, and it is not clear how the
definition should be amended. (Requiring a node to always to respond, even if it has failed, would
be nonsensical.)

\subsubsection{Finite and infinite partitions}

Even if we did redefine availability such that the ABD algorithm is ``unavailable,'' and thus not an
acceptable counterexample, the proof of Theorem 1 would still be problematic.

The proof rests on constructing an execution of an algorithm $A$ in which a write is followed by a
read, while simultaneously a partition exists in the network. However, the algorithm $A$ may never
exhibit such an execution: for example, the algorithm could simply wait until the partition is
healed before allowing the write operation to complete. With any partition of finite duration, $A$
would still satisfy the availability property, because the operations would eventually complete. In
the asynchronous model, which does not make timing assumptions, such waiting is in fact expected.
The proof is therefore false with regard to partitions of finite duration.

Only if the partition is of infinite duration, $A$ is forced to make a choice\footnote{How the
algorithm would make that choice in the absence of clocks -- since this is an asynchronous system
model -- is unclear.} between waiting until the partition heals (which never happens, thus violating
availability) and exhibiting the execution in the proof of Theorem 1 (thus violating
linearizability). We can conclude that the proof of Theorem 1 is only valid in the case of an
infinite partition.

As previously mentioned, infinite partitions cannot occur in fair-loss links, and are usually
considered a Byzantine failure mode. As discussions of CAP are usually in the context of
non-Byzantine system models, we might conclude that Theorem 1 is false in the context in which it is
normally applied.

\subsubsection{Linearizability vs. eventual consistency}

Note that in the case of an infinite partition, no information can ever flow from one sub-network to
the other. Thus, even eventual consistency (replica convergence in finite time, see
section~\ref{sec:c-in-cap}) is not possible in a system with an infinite partition, if availability
is also required.

The CAP theorem is often understood as demonstrating that linearizability cannot be achieved with
high availability, whereas eventual consistency can. In this section we showed that in the only
situation where the proof of Theorem 1 applies (an infinite partition), linearizable and eventually
consistent algorithms \emph{both} violate the availability requirement.

Thus, Theorem 1 does not serve to differentiate between the availability of linearizable and
eventually consistent systems. The common interpretation, namely that Theorem 1 proves the existence
of a trade-off between availability and consistency, is false.

\subsection{Corollary 1.1}\label{sec:corollary}

Gilbert and Lynch's Corollary~1.1 is stated as follows:

\emph{It is impossible in the asynchronous network model to implement a read/write data object that
guarantees the following properties:}
\begin{itemize}
\item \emph{Availability, in all fair executions,}
\item \emph{Atomic consistency, in fair executions in which \emph{no} messages are lost.}
\end{itemize}

The proof of Corollary~1.1 relies on Theorem~1 in a situation where no messages are lost. However,
we showed that the proof of Theorem~1 is only true in the case of an infinite partition, in which
case an infinite number of messages are lost. Thus, Corollary~1.1 is false.

\subsection{Theorem 2}\label{sec:theorem2}

Gilbert and Lynch's Theorem~2 is similar to Theorem~1, except that it is stated in the partially
synchronous network model rather than the asynchronous model.

Theorem~2 is false for the same reasons as Theorem~1: the ABD algorithm provides a counterexample,
and the execution described in the proof need not be exhibited by any algorithm in the case of a
partition of finite duration. For a partition of infinite duration, the aforementioned observation
about eventual consistency holds.

% \subsection{Definition 3 and Theorem 4}\label{sec:theorem4}
%
% Gilbert and Lynch's Definition~3 describes a new replica consistency model called \emph{Delayed-$t$
% consistency}, and a replication algorithm called \emph{modified centralized algorithm}. Theorem~4 is
% stated as follows:
%
% \emph{The modified centralized algorithm is Delayed-$t$ consistent.}
%
% The Delayed-$t$ model is similar to linearizability, except that it falls back to sequential
% consistency in the case where messages are lost. The sequential consistency property is encoded in
% requirement 3 of Definition 3, which states ``The order in $P$ is consistent with the order of
% \emph{read} and \emph{write} requests submitted at each node''.

\section{Alternatives to CAP}\label{sec:alternatives}

TODO: In this section, present alternative theoretical results that can help practitioners reason
about the design of their systems.

% The intuition we want to capture is that a partition-tolerant system should allow the response
% time (latency) of read/write requests to be lower than the duration of a partition, whereas the
% time until replica convergence is allowed to exceed the duration of a partition. However, CAP does
% not capture this intuition, since it does not consider the latency of operations. We will return
% to this in section~\ref{sec:alternatives}.

Idea: distinguish consistency models by whether an operation requires network communication to
complete before it can return. This framework focuses on operation latency as a function of network
latency; disconnected operation is modelled with very high latency. Linearizability requires waiting
on network for both reads and writes~\cite{Attiya1994gw}; sequential consistency requires waiting on
network for writes, but not for reads~\cite{Attiya1994gw, Lipton1988uh}; causal consistency (and
weaker) never require waiting on network~\cite{Mahajan2011wz}.

Random notes follow:

Linearizability is a very strong consistency guarantee: most multicore CPUs do not provide
linearizable access to memory, unless explicit memory barrier or lock instructions are
used~\cite{Sewell2010fj}. The reason is performance: even though a multicore computer is not a
distributed system, the latency of accessing RAM is so high that it would stall a CPU core for an
unacceptably long amount of time.

Abadi~\cite{Abadi2012hb} argues that the consistency/latency trade-off (which applies even when
there is no partition) is as important as the consistency/availability trade-off (which only applies
when there is a partition).

Latency percentiles are a popular metric for the performance of a service: for example, a service
level agreement may specify that 99.9\% of requests must return a response within 500~ms.

disconnected operation: ATM network~\cite{Brewer2012tr}, file systems~\cite{Kistler1992bt}.

% Brewer states that ``there is little reason to forfeit C or A when the system is not
% partitioned''~\cite{Brewer2012ba}, yet many distributed datastores (e.g.\ Dynamo, Riak, Cassandra
% and Voldemort) don't provide linearizabile consistency, even in the absence of
% partitions~\cite{Abadi2012hb}. The reduced consistency allows these systems to offer lower latency,
% and avoids the difficult problem of detecting whether the cluster is currently partitioned (which
% would be required in order to have a ``partition mode'' that is distinct from normal operation).
% Thus, these datastores that optimize for availability in the CAP sense can be thought to be always
% operating in ``partition mode''.

Even though the CAP Theorem is false (so it is not a theorem), the informally defined CAP principle
may still be useful for designing practical systems.

Impossibility results are common in distributed systems~\cite{Lynch1989kj}, and there are a number
of results that are 

\cite{Attiya1995bm}
\cite{Fischer1985tt}

Mahajan, Alvisi and Dahlin~\cite{Mahajan2011wz} show that causal consistency with convergence is the
strongest possible consistency guarantee in a system with disconnected operation.


\section{Safety and liveness}

Eventual consistency (convergence) is a liveness property: if a system stops accepting writes and
sufficient communication occurs, then eventually the system reaches a state in which reads on all
nodes return the same value for the same object~\cite{Mahajan2011wz}.

Responding eventually to a request is a liveness property, but responding within some timeout is a
safety property.

{\footnotesize
\bibliographystyle{plainnat}
\bibliography{references}{}}
\end{document}
